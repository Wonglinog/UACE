# @package _global_
# changes hyper-parameters of model if resnet-50 used with cifar-10
# more info: https://hydra.cc/docs/patterns/specializing_config
#model:
  #num_classes: 10

hparams:
  epochs: 120
  batch_size: 128
  grad_clip_max_norm: 500 # #5 # change to float to activate

  optimizer:
    _target_: torch.optim.SGD
    lr: 0.01
    weight_decay: 1e-4
    momentum: 0.9
    nesterov: false

  scheduler:
    _target_: torch.optim.lr_scheduler.CosineAnnealingLR
    T_max: 120
    eta_min: 0

  # scheduler:
  #   _target_: torch.optim.lr_scheduler.MultiStepLR
  #   milestones: [80, 120]
  #   gamma: 0.1
